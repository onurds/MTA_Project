{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99aabc27",
   "metadata": {},
   "source": [
    "# Baseline Relation Extraction for MTA Transit Alerts\n",
    "\n",
    "In this notebook i implement a rule based relation extraction system that pairs DIRECTION and ROUTE entities using segment based logic.\n",
    "\n",
    "## Schema of the extraction\n",
    "\n",
    "The system works with the following data structure:\n",
    "- **affected_spans**: `[{\"id\": 0, \"start\": X, \"end\": Y, \"type\": \"ROUTE\", \"value\": \"Q\"}, ...]`\n",
    "- **direction_spans**: `[{\"id\": 0, \"start\": X, \"end\": Y, \"type\": \"DIRECTION\", \"value\": \"SOUTHBOUND\"}, ...]`\n",
    "- **relations**: `[{\"route_span_id\": 0, \"direction_span_id\": 0, \"type\": \"HAS_DIRECTION\"}, ...]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09293ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1071a72",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing\n",
    "\n",
    "Functions to load the silver dataset and parse JSON span annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c679ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the silver dataset with direction and route spans\n",
    "def load_silver_data(filepath: str) -> pd.DataFrame:\n",
    "    print(f\"Loading data from {filepath}...\")\n",
    "    df = pd.read_csv(filepath)\n",
    "    print(f\"Loaded {len(df):,} records\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63e270f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse JSON spans string into list of span dictionaries\n",
    "def parse_spans(spans_json: str) -> List[Dict]:\n",
    "    if pd.isna(spans_json) or spans_json == '[]':\n",
    "        return []\n",
    "    try:\n",
    "        return json.loads(spans_json)\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b23d33b",
   "metadata": {},
   "source": [
    "## 2. Span Processing Utilities\n",
    "\n",
    "Helper functions to manage span IDs and identify segment breaks in text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f19c80a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add sequential IDs to spans if not already present\n",
    "def add_span_ids(spans: List[Dict], start_id: int = 0) -> List[Dict]:\n",
    "    result = []\n",
    "    for i, span in enumerate(spans):\n",
    "        span_with_id = span.copy()\n",
    "        if 'id' not in span_with_id:\n",
    "            span_with_id['id'] = start_id + i\n",
    "        result.append(span_with_id)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120fd699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if text between two entities contains a major segment break\n",
    "# Major breaks are newlines, parentheses, colons (not followed by time pattern)\n",
    "def is_major_break(text_between: str) -> bool:\n",
    "    # Check for newline\n",
    "    if '\\n' in text_between:\n",
    "        return True\n",
    "    \n",
    "    # Check for parentheses\n",
    "    if '(' in text_between or ')' in text_between:\n",
    "        return True\n",
    "    \n",
    "    # Check for colon not followed by time pattern\n",
    "    # Time pattern: colon followed by 2 digits (\"8:45\", \"10:30\")\n",
    "    colon_matches = list(re.finditer(r':', text_between))\n",
    "    for match in colon_matches:\n",
    "        after_colon = text_between[match.end():]\n",
    "        # If colon is NOT followed by time pattern (digits), it's a major break\n",
    "        if not re.match(r'\\s*\\d{2}', after_colon):\n",
    "            return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf46a7f",
   "metadata": {},
   "source": [
    "## 3. Relation Extraction Logic\n",
    "\n",
    "Core function that implements the rule-based relation extraction algorithm.\n",
    "\n",
    "### Algorithm Overview:\n",
    "1. **First pass**: Process entities left-to-right, tracking active direction\n",
    "   - DIRECTION entities set the active direction\n",
    "   - ROUTE entities pair with the current active direction\n",
    "   - Major breaks reset the active direction\n",
    "2. **Second pass**: Handle unpaired routes by looking forward for directions in the same segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8741be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_relations(\n",
    "    header: str, \n",
    "    direction_spans: List[Dict], \n",
    "    route_spans: List[Dict]\n",
    ") -> Tuple[List[Dict], List[Dict], List[Dict]]:\n",
    "    # Add IDs to spans\n",
    "    route_spans_with_ids = add_span_ids(route_spans, start_id=0)\n",
    "    direction_spans_with_ids = add_span_ids(direction_spans, start_id=0)\n",
    "    \n",
    "    if not direction_spans_with_ids or not route_spans_with_ids:\n",
    "        return route_spans_with_ids, direction_spans_with_ids, []\n",
    "    \n",
    "    # Create lookup maps for quick ID access\n",
    "    route_id_map = {(s['start'], s['end']): s['id'] for s in route_spans_with_ids}\n",
    "    direction_id_map = {(s['start'], s['end']): s['id'] for s in direction_spans_with_ids}\n",
    "    \n",
    "    # Merge all entities for sequential processing\n",
    "    entities = []\n",
    "    for span in direction_spans_with_ids:\n",
    "        entities.append({\n",
    "            'start': span['start'],\n",
    "            'end': span['end'],\n",
    "            'id': span['id'],\n",
    "            'value': span['value'],\n",
    "            'entity_type': 'DIRECTION'\n",
    "        })\n",
    "    for span in route_spans_with_ids:\n",
    "        entities.append({\n",
    "            'start': span['start'],\n",
    "            'end': span['end'],\n",
    "            'id': span['id'],\n",
    "            'value': span['value'],\n",
    "            'entity_type': 'ROUTE'\n",
    "        })\n",
    "    \n",
    "    # Sort by start position\n",
    "    entities.sort(key=lambda x: x['start'])\n",
    "    \n",
    "    relations = []\n",
    "    active_direction_id = None\n",
    "    active_direction_end = None\n",
    "    \n",
    "    # Track route_id - direction_id pairings\n",
    "    route_direction_pairs = {}\n",
    "    \n",
    "    # Track unpaired routes for second pass\n",
    "    unpaired_routes = []\n",
    "    \n",
    "    for i, entity in enumerate(entities):\n",
    "        # Check for major break from previous entity\n",
    "        if i > 0:\n",
    "            prev_entity = entities[i - 1]\n",
    "            text_between = header[prev_entity['end']:entity['start']]\n",
    "            \n",
    "            if is_major_break(text_between):\n",
    "                active_direction_id = None\n",
    "                active_direction_end = None\n",
    "        \n",
    "        if entity['entity_type'] == 'DIRECTION':\n",
    "            # Update active direction\n",
    "            active_direction_id = entity['id']\n",
    "            active_direction_end = entity['end']\n",
    "        \n",
    "        elif entity['entity_type'] == 'ROUTE':\n",
    "            route_id = entity['id']\n",
    "            \n",
    "            if active_direction_id is not None:\n",
    "                route_direction_pairs[route_id] = active_direction_id\n",
    "            else:\n",
    "                # Track unpaired route for second pass\n",
    "                unpaired_routes.append({\n",
    "                    'route_id': route_id,\n",
    "                    'route_start': entity['start'],\n",
    "                    'route_end': entity['end']\n",
    "                })\n",
    "    \n",
    "    # Second pass: assign unpaired routes to the next direction in the same segment\n",
    "    # This handles patterns like \"L trains are delayed in both directions\"\n",
    "    for unpaired in unpaired_routes:\n",
    "        route_id = unpaired['route_id']\n",
    "        route_end = unpaired['route_end']\n",
    "        \n",
    "        # Find the next direction after this route\n",
    "        for entity in entities:\n",
    "            if entity['entity_type'] == 'DIRECTION' and entity['start'] > route_end:\n",
    "                # Check if there's a segment break between route and this direction\n",
    "                text_between = header[route_end:entity['start']]\n",
    "                if not is_major_break(text_between):\n",
    "                    route_direction_pairs[route_id] = entity['id']\n",
    "                    break\n",
    "                else:\n",
    "                    # Hit a segment break, stop looking\n",
    "                    break\n",
    "    \n",
    "    # Convert pairs to relation format\n",
    "    for route_id, direction_id in route_direction_pairs.items():\n",
    "        relations.append({\n",
    "            'route_span_id': route_id,\n",
    "            'direction_span_id': direction_id,\n",
    "            'type': 'HAS_DIRECTION'\n",
    "        })\n",
    "    \n",
    "    # Sort relations by route_span_id for consistent output\n",
    "    relations.sort(key=lambda x: x['route_span_id'])\n",
    "    \n",
    "    return route_spans_with_ids, direction_spans_with_ids, relations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32c6dba",
   "metadata": {},
   "source": [
    "## 4. Dataset Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4cdf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the silver dataset and extract relations for each row\n",
    "# Updates spans to include IDs and adds relations column\n",
    "def process_dataset(input_path: str, output_path: str) -> pd.DataFrame:\n",
    "    df = load_silver_data(input_path)\n",
    "    \n",
    "    print(\"Extracting relations...\")\n",
    "    updated_route_spans_list = []\n",
    "    updated_direction_spans_list = []\n",
    "    relations_list = []\n",
    "    relation_names_list = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        if (idx + 1) % 50000 == 0:\n",
    "            print(f\"  Processed {idx + 1:,} / {len(df):,} records...\")\n",
    "        \n",
    "        header = str(row['header']) if pd.notna(row['header']) else \"\"\n",
    "        direction_spans = parse_spans(row.get('direction_spans', '[]'))\n",
    "        route_spans = parse_spans(row.get('affected_spans', '[]'))\n",
    "        \n",
    "        updated_routes, updated_directions, relations = extract_relations(\n",
    "            header, direction_spans, route_spans\n",
    "        )\n",
    "        \n",
    "        # Create relation names by looking up the actual route and direction values\n",
    "        relation_names = []\n",
    "        # Build lookup maps\n",
    "        route_id_to_value = {s['id']: s['value'] for s in updated_routes}\n",
    "        direction_id_to_value = {s['id']: s['value'] for s in updated_directions}\n",
    "        \n",
    "        for rel in relations:\n",
    "            route_value = route_id_to_value.get(rel['route_span_id'], 'UNKNOWN')\n",
    "            direction_value = direction_id_to_value.get(rel['direction_span_id'], 'UNKNOWN')\n",
    "            relation_names.append({\n",
    "                'route': route_value,\n",
    "                'direction': direction_value\n",
    "            })\n",
    "        \n",
    "        updated_route_spans_list.append(json.dumps(updated_routes))\n",
    "        updated_direction_spans_list.append(json.dumps(updated_directions))\n",
    "        relation_names_list.append(json.dumps(relation_names))\n",
    "        relations_list.append(json.dumps(relations))\n",
    "    \n",
    "    # Update dataframe with ID enhanced spans and relations\n",
    "    df['affected_spans'] = updated_route_spans_list\n",
    "    df['direction_spans'] = updated_direction_spans_list\n",
    "    df['relation_names'] = relation_names_list\n",
    "    df['relations'] = relations_list\n",
    "    \n",
    "    print(f\"\\nWriting output to {output_path}...\")\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"Successfully wrote {len(df):,} records to {output_path}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea25102",
   "metadata": {},
   "source": [
    "## 5. Statistical Analysis\n",
    "\n",
    "Generate statistics on the extracted relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df35303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print compact EDA statistics for the extracted relations\n",
    "def print_eda_stats(df: pd.DataFrame):\n",
    "    print(\"Stats\")\n",
    "    \n",
    "    # Parse relations and spans for analysis\n",
    "    relations_counts = []\n",
    "    direction_type_counts = defaultdict(int)\n",
    "    total_relations = 0\n",
    "    rows_with_relations = 0\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        relations = json.loads(row['relations']) if row['relations'] else []\n",
    "        direction_spans = json.loads(row['direction_spans']) if row['direction_spans'] else []\n",
    "        \n",
    "        # Build direction_id -> value map\n",
    "        dir_id_to_value = {s['id']: s['value'] for s in direction_spans}\n",
    "        \n",
    "        count = len(relations)\n",
    "        relations_counts.append(count)\n",
    "        total_relations += count\n",
    "        \n",
    "        if count > 0:\n",
    "            rows_with_relations += 1\n",
    "        \n",
    "        for rel in relations:\n",
    "            direction_id = rel['direction_span_id']\n",
    "            direction_value = dir_id_to_value.get(direction_id, 'UNKNOWN')\n",
    "            direction_type_counts[direction_value] += 1\n",
    "    \n",
    "    # Basic stats\n",
    "    print(f\"\\nTotal rows: {len(df):,}\")\n",
    "    print(f\"Rows with relations: {rows_with_relations:,} ({100*rows_with_relations/len(df):.1f}%)\")\n",
    "    print(f\"Total relation pairs: {total_relations:,}\")\n",
    "    \n",
    "    # Single vs multi relation distribution\n",
    "    single_relation = sum(1 for c in relations_counts if c == 1)\n",
    "    multi_relation = sum(1 for c in relations_counts if c > 1)\n",
    "    print(f\"\\nSingle relation rows: {single_relation:,}\")\n",
    "    print(f\"Multi relation rows: {multi_relation:,}\")\n",
    "    \n",
    "    # Direction type distribution\n",
    "    print(f\"\\nDirection Type Distribution:\")\n",
    "    for direction, count in sorted(direction_type_counts.items(), key=lambda x: -x[1]):\n",
    "        pct = 100 * count / total_relations if total_relations > 0 else 0\n",
    "        print(f\"  {direction}: {count:,} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be400c3",
   "metadata": {},
   "source": [
    "## 6. Main Execution\n",
    "\n",
    "Run the baseline relation extraction pipeline on the silver dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "162be12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from Preprocessed/MTA_Data_silver_directions.csv...\n",
      "Loaded 226,160 records\n",
      "Extracting relations...\n",
      "  Processed 50,000 / 226,160 records...\n",
      "  Processed 100,000 / 226,160 records...\n",
      "  Processed 150,000 / 226,160 records...\n",
      "  Processed 200,000 / 226,160 records...\n",
      "\n",
      "Writing output to Preprocessed/MTA_Data_silver_relations.csv...\n",
      "Successfully wrote 226,160 records to Preprocessed/MTA_Data_silver_relations.csv\n",
      "\n",
      "============================================================\n",
      "RELATION EXTRACTION STATISTICS\n",
      "============================================================\n",
      "\n",
      "Total rows: 226,160\n",
      "Rows with relations: 179,548 (79.4%)\n",
      "Total relation pairs: 293,474\n",
      "\n",
      "Single-relation rows: 102,198\n",
      "Multi-relation rows: 77,350\n",
      "\n",
      "Direction Type Distribution:\n",
      "  SOUTHBOUND: 89,350 (30.4%)\n",
      "  NORTHBOUND: 87,501 (29.8%)\n",
      "  BOTH_DIRECTIONS: 63,035 (21.5%)\n",
      "  PLACE_BOUND: 27,173 (9.3%)\n",
      "  UPTOWN: 4,746 (1.6%)\n",
      "  DOWNTOWN: 4,668 (1.6%)\n",
      "  WESTBOUND: 4,091 (1.4%)\n",
      "  EASTBOUND: 3,978 (1.4%)\n",
      "  MANHATTAN_BOUND: 3,784 (1.3%)\n",
      "  BROOKLYN_BOUND: 2,059 (0.7%)\n",
      "  QUEENS_BOUND: 1,923 (0.7%)\n",
      "  BRONX_BOUND: 898 (0.3%)\n",
      "  STATENISLAND_BOUND: 268 (0.1%)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "input_path = 'Preprocessed/MTA_Data_silver_directions.csv'\n",
    "output_path = 'Preprocessed/MTA_Data_silver_relations.csv'\n",
    "\n",
    "df = process_dataset(input_path, output_path)\n",
    "print_eda_stats(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
