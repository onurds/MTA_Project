{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99aabc27",
   "metadata": {},
   "source": [
    "# Baseline Relation Extraction for MTA Transit Alerts\n",
    "\n",
    "This notebook implements a rule-based relation extraction system that pairs DIRECTION and ROUTE entities using segment-based logic with direction inheritance.\n",
    "\n",
    "## Schema\n",
    "\n",
    "The system works with the following data structure:\n",
    "- **affected_spans**: `[{\"id\": 0, \"start\": X, \"end\": Y, \"type\": \"ROUTE\", \"value\": \"Q\"}, ...]`\n",
    "- **direction_spans**: `[{\"id\": 0, \"start\": X, \"end\": Y, \"type\": \"DIRECTION\", \"value\": \"SOUTHBOUND\"}, ...]`\n",
    "- **relations**: `[{\"route_span_id\": 0, \"direction_span_id\": 0, \"type\": \"HAS_DIRECTION\"}, ...]`\n",
    "\n",
    "## Extraction Rules\n",
    "\n",
    "1. **Direction inheritance**: Active direction persists until a new direction is found\n",
    "2. **Major segment breaks**: Reset active direction on newlines, parentheses, or colons (not followed by time)\n",
    "3. **Both directions pattern**: Handles cases where direction comes after route (e.g., \"L trains in both directions\")\n",
    "4. **Sequential processing**: Left-to-right processing of merged entity spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09293ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1071a72",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing\n",
    "\n",
    "Functions to load the silver dataset and parse JSON span annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c679ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the silver dataset with direction and route spans\n",
    "def load_silver_data(filepath: str) -> pd.DataFrame:\n",
    "    print(f\"Loading data from {filepath}...\")\n",
    "    df = pd.read_csv(filepath)\n",
    "    print(f\"Loaded {len(df):,} records\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e270f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse JSON spans string into list of span dictionaries\n",
    "def parse_spans(spans_json: str) -> List[Dict]:\n",
    "    if pd.isna(spans_json) or spans_json == '[]':\n",
    "        return []\n",
    "    try:\n",
    "        return json.loads(spans_json)\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b23d33b",
   "metadata": {},
   "source": [
    "## 2. Span Processing Utilities\n",
    "\n",
    "Helper functions to manage span IDs and identify segment breaks in text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19c80a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add sequential IDs to spans if not already present\n",
    "def add_span_ids(spans: List[Dict], start_id: int = 0) -> List[Dict]:\n",
    "    result = []\n",
    "    for i, span in enumerate(spans):\n",
    "        span_with_id = span.copy()\n",
    "        if 'id' not in span_with_id:\n",
    "            span_with_id['id'] = start_id + i\n",
    "        result.append(span_with_id)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120fd699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if text between two entities contains a major segment break\n",
    "# Major breaks: newlines, parentheses, colons (not followed by time pattern)\n",
    "def is_major_break(text_between: str) -> bool:\n",
    "    # Check for newline\n",
    "    if '\\n' in text_between:\n",
    "        return True\n",
    "    \n",
    "    # Check for parentheses\n",
    "    if '(' in text_between or ')' in text_between:\n",
    "        return True\n",
    "    \n",
    "    # Check for colon not followed by time pattern\n",
    "    # Time pattern: colon followed by 2 digits (e.g., \"8:45\", \"10:30\")\n",
    "    colon_matches = list(re.finditer(r':', text_between))\n",
    "    for match in colon_matches:\n",
    "        after_colon = text_between[match.end():]\n",
    "        # If colon is NOT followed by time pattern (digits), it's a major break\n",
    "        if not re.match(r'\\s*\\d{2}', after_colon):\n",
    "            return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf46a7f",
   "metadata": {},
   "source": [
    "## 3. Relation Extraction Logic\n",
    "\n",
    "Core function that implements the rule-based relation extraction algorithm.\n",
    "\n",
    "### Algorithm Overview:\n",
    "1. **First pass**: Process entities left-to-right, tracking active direction\n",
    "   - DIRECTION entities set the active direction\n",
    "   - ROUTE entities pair with the current active direction\n",
    "   - Major breaks reset the active direction\n",
    "2. **Second pass**: Handle unpaired routes by looking forward for directions in the same segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8741be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_relations(\n",
    "    header: str, \n",
    "    direction_spans: List[Dict], \n",
    "    route_spans: List[Dict]\n",
    ") -> Tuple[List[Dict], List[Dict], List[Dict]]:\n",
    "    # Add IDs to spans\n",
    "    route_spans_with_ids = add_span_ids(route_spans, start_id=0)\n",
    "    direction_spans_with_ids = add_span_ids(direction_spans, start_id=0)\n",
    "    \n",
    "    if not direction_spans_with_ids or not route_spans_with_ids:\n",
    "        return route_spans_with_ids, direction_spans_with_ids, []\n",
    "    \n",
    "    # Create lookup maps for quick ID access\n",
    "    route_id_map = {(s['start'], s['end']): s['id'] for s in route_spans_with_ids}\n",
    "    direction_id_map = {(s['start'], s['end']): s['id'] for s in direction_spans_with_ids}\n",
    "    \n",
    "    # Merge all entities for sequential processing\n",
    "    entities = []\n",
    "    for span in direction_spans_with_ids:\n",
    "        entities.append({\n",
    "            'start': span['start'],\n",
    "            'end': span['end'],\n",
    "            'id': span['id'],\n",
    "            'value': span['value'],\n",
    "            'entity_type': 'DIRECTION'\n",
    "        })\n",
    "    for span in route_spans_with_ids:\n",
    "        entities.append({\n",
    "            'start': span['start'],\n",
    "            'end': span['end'],\n",
    "            'id': span['id'],\n",
    "            'value': span['value'],\n",
    "            'entity_type': 'ROUTE'\n",
    "        })\n",
    "    \n",
    "    # Sort by start position\n",
    "    entities.sort(key=lambda x: x['start'])\n",
    "    \n",
    "    relations = []\n",
    "    active_direction_id = None\n",
    "    active_direction_end = None\n",
    "    \n",
    "    # Track route_id -> direction_id pairings\n",
    "    route_direction_pairs = {}\n",
    "    \n",
    "    # Track unpaired routes for second pass\n",
    "    unpaired_routes = []\n",
    "    \n",
    "    for i, entity in enumerate(entities):\n",
    "        # Check for major break from previous entity\n",
    "        if i > 0:\n",
    "            prev_entity = entities[i - 1]\n",
    "            text_between = header[prev_entity['end']:entity['start']]\n",
    "            \n",
    "            if is_major_break(text_between):\n",
    "                active_direction_id = None\n",
    "                active_direction_end = None\n",
    "        \n",
    "        if entity['entity_type'] == 'DIRECTION':\n",
    "            # Update active direction\n",
    "            active_direction_id = entity['id']\n",
    "            active_direction_end = entity['end']\n",
    "        \n",
    "        elif entity['entity_type'] == 'ROUTE':\n",
    "            route_id = entity['id']\n",
    "            \n",
    "            if active_direction_id is not None:\n",
    "                route_direction_pairs[route_id] = active_direction_id\n",
    "            else:\n",
    "                # Track unpaired route for second pass\n",
    "                unpaired_routes.append({\n",
    "                    'route_id': route_id,\n",
    "                    'route_start': entity['start'],\n",
    "                    'route_end': entity['end']\n",
    "                })\n",
    "    \n",
    "    # Second pass: assign unpaired routes to the next direction in the same segment\n",
    "    # This handles \"L trains are delayed in both directions\" pattern\n",
    "    for unpaired in unpaired_routes:\n",
    "        route_id = unpaired['route_id']\n",
    "        route_end = unpaired['route_end']\n",
    "        \n",
    "        # Find the next direction after this route\n",
    "        for entity in entities:\n",
    "            if entity['entity_type'] == 'DIRECTION' and entity['start'] > route_end:\n",
    "                # Check if there's a segment break between route and this direction\n",
    "                text_between = header[route_end:entity['start']]\n",
    "                if not is_major_break(text_between):\n",
    "                    route_direction_pairs[route_id] = entity['id']\n",
    "                    break\n",
    "                else:\n",
    "                    # Hit a segment break, stop looking\n",
    "                    break\n",
    "    \n",
    "    # Convert pairs to relation format\n",
    "    for route_id, direction_id in route_direction_pairs.items():\n",
    "        relations.append({\n",
    "            'route_span_id': route_id,\n",
    "            'direction_span_id': direction_id,\n",
    "            'type': 'HAS_DIRECTION'\n",
    "        })\n",
    "    \n",
    "    # Sort relations by route_span_id for consistent output\n",
    "    relations.sort(key=lambda x: x['route_span_id'])\n",
    "    \n",
    "    return route_spans_with_ids, direction_spans_with_ids, relations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32c6dba",
   "metadata": {},
   "source": [
    "## 4. Dataset Processing\n",
    "\n",
    "Process the entire silver dataset and extract relations for all rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4cdf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the silver dataset and extract relations for each row\n",
    "# Updates spans to include IDs and adds relations column\n",
    "def process_dataset(input_path: str, output_path: str) -> pd.DataFrame:\n",
    "    df = load_silver_data(input_path)\n",
    "    \n",
    "    print(\"Extracting relations...\")\n",
    "    updated_route_spans_list = []\n",
    "    updated_direction_spans_list = []\n",
    "    relations_list = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        if (idx + 1) % 50000 == 0:\n",
    "            print(f\"  Processed {idx + 1:,} / {len(df):,} records...\")\n",
    "        \n",
    "        header = str(row['header']) if pd.notna(row['header']) else \"\"\n",
    "        direction_spans = parse_spans(row.get('direction_spans', '[]'))\n",
    "        route_spans = parse_spans(row.get('affected_spans', '[]'))\n",
    "        \n",
    "        updated_routes, updated_directions, relations = extract_relations(\n",
    "            header, direction_spans, route_spans\n",
    "        )\n",
    "        \n",
    "        updated_route_spans_list.append(json.dumps(updated_routes))\n",
    "        updated_direction_spans_list.append(json.dumps(updated_directions))\n",
    "        relations_list.append(json.dumps(relations))\n",
    "    \n",
    "    # Update dataframe with ID-enhanced spans and relations\n",
    "    df['affected_spans'] = updated_route_spans_list\n",
    "    df['direction_spans'] = updated_direction_spans_list\n",
    "    df['relations'] = relations_list\n",
    "    \n",
    "    print(f\"\\nWriting output to {output_path}...\")\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"Successfully wrote {len(df):,} records to {output_path}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea25102",
   "metadata": {},
   "source": [
    "## 5. Statistical Analysis\n",
    "\n",
    "Generate comprehensive statistics on the extracted relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df35303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print compact EDA statistics for the extracted relations\n",
    "def print_eda_stats(df: pd.DataFrame):\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"RELATION EXTRACTION STATISTICS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Parse relations and spans for analysis\n",
    "    relations_counts = []\n",
    "    direction_type_counts = defaultdict(int)\n",
    "    total_relations = 0\n",
    "    rows_with_relations = 0\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        relations = json.loads(row['relations']) if row['relations'] else []\n",
    "        direction_spans = json.loads(row['direction_spans']) if row['direction_spans'] else []\n",
    "        \n",
    "        # Build direction_id -> value map\n",
    "        dir_id_to_value = {s['id']: s['value'] for s in direction_spans}\n",
    "        \n",
    "        count = len(relations)\n",
    "        relations_counts.append(count)\n",
    "        total_relations += count\n",
    "        \n",
    "        if count > 0:\n",
    "            rows_with_relations += 1\n",
    "        \n",
    "        for rel in relations:\n",
    "            direction_id = rel['direction_span_id']\n",
    "            direction_value = dir_id_to_value.get(direction_id, 'UNKNOWN')\n",
    "            direction_type_counts[direction_value] += 1\n",
    "    \n",
    "    # Basic stats\n",
    "    print(f\"\\nTotal rows: {len(df):,}\")\n",
    "    print(f\"Rows with relations: {rows_with_relations:,} ({100*rows_with_relations/len(df):.1f}%)\")\n",
    "    print(f\"Total relation pairs: {total_relations:,}\")\n",
    "    \n",
    "    # Single vs multi-relation distribution\n",
    "    single_relation = sum(1 for c in relations_counts if c == 1)\n",
    "    multi_relation = sum(1 for c in relations_counts if c > 1)\n",
    "    print(f\"\\nSingle-relation rows: {single_relation:,}\")\n",
    "    print(f\"Multi-relation rows: {multi_relation:,}\")\n",
    "    \n",
    "    # Direction type distribution\n",
    "    print(f\"\\nDirection Type Distribution:\")\n",
    "    for direction, count in sorted(direction_type_counts.items(), key=lambda x: -x[1]):\n",
    "        pct = 100 * count / total_relations if total_relations > 0 else 0\n",
    "        print(f\"  {direction}: {count:,} ({pct:.1f}%)\")\n",
    "    \n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be400c3",
   "metadata": {},
   "source": [
    "## 6. Main Execution\n",
    "\n",
    "Run the baseline relation extraction pipeline on the silver dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162be12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution: baseline relation extraction\n",
    "input_path = 'Preprocessed/MTA_Data_silver_directions.csv'\n",
    "output_path = 'Preprocessed/MTA_Data_silver_relations.csv'\n",
    "\n",
    "df = process_dataset(input_path, output_path)\n",
    "print_eda_stats(df)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
