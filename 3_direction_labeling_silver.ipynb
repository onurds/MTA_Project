{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MTA Direction Auto Labeler for NER and RE Training\n",
        "\n",
        "In this notebook i auto label directions in MTA bus and subway alerts using rule based pattern matching.\n",
        "\n",
        "## Label Types:\n",
        "- **COMPASS**: NORTHBOUND, SOUTHBOUND, EASTBOUND, WESTBOUND\n",
        "- **BOROUGH**: MANHATTAN_BOUND, QUEENS_BOUND, BRONX_BOUND, BROOKLYN_BOUND, STATENISLAND_BOUND\n",
        "- **LOCAL**: UPTOWN, DOWNTOWN\n",
        "- **PLACE_BOUND**: Any other location based direction\n",
        "- **BOTH_DIRECTIONS**: Both/either direction phrases\n",
        "- **UNSPECIFIED**: No direction detected\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import csv\n",
        "import json\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Direction Classifier\n",
        "\n",
        "Classifies detected direction targets into compass, borough, or place-bound labels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DirectionClassifier:\n",
        "    # Classifies direction targets into COMPASS, BOROUGH, LOCAL, or PLACE_BOUND\n",
        "    \n",
        "    COMPASS_TERMS = {'north', 'south', 'east', 'west'}\n",
        "    BOROUGH_TERMS = {'manhattan', 'queens', 'bronx', 'brooklyn', 'staten island'}\n",
        "    LOCAL_TERMS = {'uptown', 'downtown'}\n",
        "    \n",
        "    COMPASS_LABELS = {\n",
        "        'north': 'NORTHBOUND',\n",
        "        'south': 'SOUTHBOUND',\n",
        "        'east': 'EASTBOUND',\n",
        "        'west': 'WESTBOUND'\n",
        "    }\n",
        "    \n",
        "    BOROUGH_LABELS = {\n",
        "        'manhattan': 'MANHATTAN_BOUND',\n",
        "        'queens': 'QUEENS_BOUND',\n",
        "        'bronx': 'BRONX_BOUND',\n",
        "        'brooklyn': 'BROOKLYN_BOUND',\n",
        "        'staten island': 'STATENISLAND_BOUND'\n",
        "    }\n",
        "    \n",
        "    LOCAL_LABELS = {\n",
        "        'uptown': 'UPTOWN',\n",
        "        'downtown': 'DOWNTOWN'\n",
        "    }\n",
        "    \n",
        "    @classmethod\n",
        "    def classify(cls, target_text):\n",
        "        # Returns normalized direction label based on precedence: COMPASS > BOROUGH > LOCAL > PLACE_BOUND\n",
        "        target_normalized = target_text.strip().lower()\n",
        "        \n",
        "        if target_normalized in cls.COMPASS_TERMS:\n",
        "            return cls.COMPASS_LABELS[target_normalized]\n",
        "        \n",
        "        if target_normalized in cls.BOROUGH_TERMS:\n",
        "            return cls.BOROUGH_LABELS[target_normalized]\n",
        "        \n",
        "        if target_normalized in cls.LOCAL_TERMS:\n",
        "            return cls.LOCAL_LABELS[target_normalized]\n",
        "        \n",
        "        return 'PLACE_BOUND'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Direction Detector\n",
        "\n",
        "Implements pattern matching rules for detecting direction mentions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DirectionDetector:\n",
        "    # Detects direction mentions in MTA alert headers\n",
        "    \n",
        "    # Rejection patterns - only reject clearly invalid patterns\n",
        "    REJECTION_PATTERNS = [\n",
        "        r'\\bin\\s+both\\s+directions?\\s+of\\b',  # \"in both direction of X\" (invalid)\n",
        "    ]\n",
        "    \n",
        "    # Stop words that signal a boundary (common grammatical words)\n",
        "    STOP_WORDS = {\n",
        "        'a', 'an', 'the',\n",
        "        'and', 'or', 'but', 'nor',\n",
        "        'to', 'from', 'via', 'at', 'in', 'on', 'for', 'of', 'with', 'by', 'as', 'into',\n",
        "        'are', 'is', 'was', 'were', 'will', 'may', 'can', 'could', 'would', 'should',\n",
        "        'be', 'been', 'being', 'have', 'has', 'had',\n",
        "        'running', 'operating', 'making', 'skipping', 'stopping', 'ending', 'starting',\n",
        "        'experience', 'expect', 'wait', 'stop', 'last', 'first', 'next',\n",
        "        'some', 'all', 'no', 'any', 'each', 'every', 'this', 'that', 'these', 'those', 'most',\n",
        "        'trains', 'train', 'buses', 'bus', 'service', 'services', 'shuttle', 'shuttles',\n",
        "        'longer', 'shorter', 'local', 'express', 'limited', 'delayed', 'suspended',\n",
        "        'you', 'your', 'we', 'our', 'they', 'their'\n",
        "    }\n",
        "    \n",
        "    # Known abbreviations that are part of place names (not boundaries)\n",
        "    KNOWN_ABBREVIATIONS = {\n",
        "        'st', 'st.', 'ave', 'av', 'av.', 'ave.', 'sq', 'sq.', 'blvd', 'blvd.',\n",
        "        'pkwy', 'pkwy.', 'rd', 'rd.', 'pl', 'pl.', 'ct', 'ct.', 'dr', 'dr.',\n",
        "        'hwy', 'hwy.', 'jct', 'jct.', 'ctr', 'ctr.', 'pk', 'pk.',\n",
        "        'sts', 'avs'\n",
        "    }\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.classifier = DirectionClassifier()\n",
        "        \n",
        "        # Borough patterns (highest priority)\n",
        "        self.borough_patterns = [\n",
        "            (re.compile(r'(?:^|(?<=[^a-zA-Z]))Manhattan[-\\s]?bound(?:\\b|(?=[A-Z0-9]))', re.IGNORECASE | re.MULTILINE), 'MANHATTAN_BOUND'),\n",
        "            (re.compile(r'(?:^|(?<=[^a-zA-Z]))Queens[-\\s]?bound(?:\\b|(?=[A-Z0-9]))', re.IGNORECASE | re.MULTILINE), 'QUEENS_BOUND'),\n",
        "            (re.compile(r'(?:^|(?<=[^a-zA-Z]))Bronx[-\\s]?bound(?:\\b|(?=[A-Z0-9]))', re.IGNORECASE | re.MULTILINE), 'BRONX_BOUND'),\n",
        "            (re.compile(r'(?:^|(?<=[^a-zA-Z]))Brooklyn[-\\s]?bound(?:\\b|(?=[A-Z0-9]))', re.IGNORECASE | re.MULTILINE), 'BROOKLYN_BOUND'),\n",
        "            (re.compile(r'(?:^|(?<=[^a-zA-Z]))Staten\\s+Island[-\\s]?bound(?:\\b|(?=[A-Z0-9]))', re.IGNORECASE | re.MULTILINE), 'STATENISLAND_BOUND'),\n",
        "        ]\n",
        "        \n",
        "        # Local direction patterns (Uptown/Downtown)\n",
        "        self.local_patterns = [\n",
        "            (re.compile(r'\\bUptown\\b', re.IGNORECASE), 'UPTOWN'),\n",
        "            (re.compile(r'\\bDowntown\\b', re.IGNORECASE), 'DOWNTOWN'),\n",
        "        ]\n",
        "        \n",
        "        # Compass direction patterns\n",
        "        self.compass_patterns = [\n",
        "            (re.compile(r'(?:^|(?<=[^a-zA-Z]))North[-\\s]?bound(?:\\b|(?=[A-Z0-9]))', re.IGNORECASE | re.MULTILINE), 'NORTHBOUND'),\n",
        "            (re.compile(r'(?:^|(?<=[^a-zA-Z]))South[-\\s]?bound(?:\\b|(?=[A-Z0-9]))', re.IGNORECASE | re.MULTILINE), 'SOUTHBOUND'),\n",
        "            (re.compile(r'(?:^|(?<=[^a-zA-Z]))East[-\\s]?bound(?:\\b|(?=[A-Z0-9]))', re.IGNORECASE | re.MULTILINE), 'EASTBOUND'),\n",
        "            (re.compile(r'(?:^|(?<=[^a-zA-Z]))West[-\\s]?bound(?:\\b|(?=[A-Z0-9]))', re.IGNORECASE | re.MULTILINE), 'WESTBOUND'),\n",
        "        ]\n",
        "        \n",
        "        # Place-bound anchor pattern: finds \"-bound\" or \" bound\"\n",
        "        self.place_bound_anchor_pattern = re.compile(r'[-\\s]bound\\b', re.IGNORECASE)\n",
        "        \n",
        "        # Both/either direction(s) pattern\n",
        "        self.both_either_pattern = re.compile(r'\\b(?:both|either)\\s+directions?\\b', re.IGNORECASE)\n",
        "    \n",
        "    def _is_abbreviation(self, word):\n",
        "        word_clean = word.lower().rstrip('.')\n",
        "        return word_clean in self.KNOWN_ABBREVIATIONS or word.lower() in self.KNOWN_ABBREVIATIONS\n",
        "    \n",
        "    def _is_stop_word(self, word):\n",
        "        return word.lower() in self.STOP_WORDS\n",
        "    \n",
        "    def _tokenize_leftward(self, text, end_pos):\n",
        "        # Tokenize text going leftward from end_pos\n",
        "        # Returns list of (start_pos, end_pos, token_text, is_connected) in left-to-right order\n",
        "        tokens = []\n",
        "        pos = end_pos\n",
        "        prev_connected = False\n",
        "        \n",
        "        while pos > 0:\n",
        "            # Skip whitespace\n",
        "            while pos > 0 and text[pos - 1] == ' ':\n",
        "                pos -= 1\n",
        "            \n",
        "            if pos == 0:\n",
        "                break\n",
        "            \n",
        "            # Check for sentence boundary\n",
        "            if text[pos - 1] in '\\n,':\n",
        "                break\n",
        "            \n",
        "            # Check for dash or slash (connector)\n",
        "            if text[pos - 1] in '-/':\n",
        "                prev_connected = True\n",
        "                pos -= 1\n",
        "                continue\n",
        "            \n",
        "            # Extract the token\n",
        "            token_end = pos\n",
        "            token_start = pos\n",
        "            \n",
        "            while token_start > 0 and (text[token_start - 1].isalnum() or text[token_start - 1] == '.'):\n",
        "                if text[token_start - 1] == '.':\n",
        "                    # Check if period is part of abbreviation\n",
        "                    temp_start = token_start - 1\n",
        "                    while temp_start > 0 and text[temp_start - 1].isalpha():\n",
        "                        temp_start -= 1\n",
        "                    potential_abbrev = text[temp_start:token_start - 1]\n",
        "                    if potential_abbrev and self._is_abbreviation(potential_abbrev):\n",
        "                        token_start = temp_start\n",
        "                        break\n",
        "                    else:\n",
        "                        break\n",
        "                token_start -= 1\n",
        "            \n",
        "            if token_start < token_end:\n",
        "                token_text = text[token_start:token_end]\n",
        "                tokens.append((token_start, token_end, token_text, prev_connected))\n",
        "                pos = token_start\n",
        "                prev_connected = False\n",
        "            else:\n",
        "                break\n",
        "        \n",
        "        tokens.reverse()\n",
        "        return tokens\n",
        "    \n",
        "    def _find_place_bound_start(self, text, bound_start):\n",
        "        # Find the start position of a place name by scanning leftward from \"bound\"\n",
        "        tokens = self._tokenize_leftward(text, bound_start)\n",
        "        \n",
        "        if not tokens:\n",
        "            return bound_start\n",
        "        \n",
        "        # Find the boundary - place name starts after the last stop word (not dash-connected)\n",
        "        place_start_idx = 0\n",
        "        \n",
        "        for i, (start, end, token_text, is_connected) in enumerate(tokens):\n",
        "            base_word = token_text.split('-')[0].split('/')[0]\n",
        "            \n",
        "            if self._is_stop_word(base_word) and not is_connected:\n",
        "                place_start_idx = i + 1\n",
        "            \n",
        "            # Handle standalone numbers\n",
        "            if base_word.isdigit() and not is_connected:\n",
        "                if i + 1 < len(tokens):\n",
        "                    next_token = tokens[i + 1][2].split('-')[0].split('/')[0]\n",
        "                    if not self._is_abbreviation(next_token):\n",
        "                        place_start_idx = i + 1\n",
        "                else:\n",
        "                    place_start_idx = i + 1\n",
        "        \n",
        "        if place_start_idx < len(tokens):\n",
        "            return tokens[place_start_idx][0]\n",
        "        else:\n",
        "            return bound_start\n",
        "    \n",
        "    def _check_rejection_context(self, text, match_start, match_end):\n",
        "        # Check if match appears in a rejected context (only checks BEFORE the match)\n",
        "        context_start = max(0, match_start - 10)\n",
        "        context = text[context_start:match_end + 5]\n",
        "        \n",
        "        for pattern in self.REJECTION_PATTERNS:\n",
        "            if re.search(pattern, context, re.IGNORECASE):\n",
        "                return True\n",
        "        return False\n",
        "    \n",
        "\n",
        "    def _has_overlap(self, start, end, detected_ranges):\n",
        "        for ds, de in detected_ranges:\n",
        "            if not (end <= ds or start >= de):\n",
        "                return True\n",
        "        return False\n",
        "    \n",
        "    def detect_place_bound_directions(self, text, detected_ranges):\n",
        "        # Detect place-bound directions with improved boundary detection\n",
        "        detections = []\n",
        "        \n",
        "        for match in self.place_bound_anchor_pattern.finditer(text):\n",
        "            bound_start = match.start()\n",
        "            bound_end = match.end()\n",
        "            \n",
        "            if self._has_overlap(bound_start, bound_end, detected_ranges):\n",
        "                continue\n",
        "            \n",
        "            place_start = self._find_place_bound_start(text, bound_start)\n",
        "            \n",
        "            if place_start >= bound_start:\n",
        "                continue\n",
        "            \n",
        "            if self._has_overlap(place_start, bound_end, detected_ranges):\n",
        "                continue\n",
        "            \n",
        "            if self._check_rejection_context(text, place_start, bound_end):\n",
        "                continue\n",
        "            \n",
        "            full_match = text[place_start:bound_end]\n",
        "            detected_ranges.append((place_start, bound_end))\n",
        "            \n",
        "            detections.append({\n",
        "                'text': full_match,\n",
        "                'start': place_start,\n",
        "                'end': bound_end,\n",
        "                'label': 'PLACE_BOUND'\n",
        "            })\n",
        "        \n",
        "        return detections\n",
        "    \n",
        "    def detect_bound_directions(self, text):\n",
        "        # Detect X-bound patterns with priority: Borough > Local > Compass > Place-bound\n",
        "        detections = []\n",
        "        detected_ranges = []\n",
        "        \n",
        "        # Priority 1: Borough directions\n",
        "        for pattern, label in self.borough_patterns:\n",
        "            for match in pattern.finditer(text):\n",
        "                start_pos, end_pos = match.start(), match.end()\n",
        "                \n",
        "                if self._has_overlap(start_pos, end_pos, detected_ranges):\n",
        "                    continue\n",
        "                if self._check_rejection_context(text, start_pos, end_pos):\n",
        "                    continue\n",
        "                \n",
        "                detected_ranges.append((start_pos, end_pos))\n",
        "                detections.append({\n",
        "                    'text': match.group(0),\n",
        "                    'start': start_pos,\n",
        "                    'end': end_pos,\n",
        "                    'label': label\n",
        "                })\n",
        "        \n",
        "        # Priority 2: Local directions\n",
        "        for pattern, label in self.local_patterns:\n",
        "            for match in pattern.finditer(text):\n",
        "                start_pos, end_pos = match.start(), match.end()\n",
        "                \n",
        "                if self._has_overlap(start_pos, end_pos, detected_ranges):\n",
        "                    continue\n",
        "                if self._check_rejection_context(text, start_pos, end_pos):\n",
        "                    continue\n",
        "                \n",
        "                detected_ranges.append((start_pos, end_pos))\n",
        "                detections.append({\n",
        "                    'text': match.group(0),\n",
        "                    'start': start_pos,\n",
        "                    'end': end_pos,\n",
        "                    'label': label\n",
        "                })\n",
        "        \n",
        "        # Priority 3: Compass directions\n",
        "        for pattern, label in self.compass_patterns:\n",
        "            for match in pattern.finditer(text):\n",
        "                start_pos, end_pos = match.start(), match.end()\n",
        "                \n",
        "                if self._has_overlap(start_pos, end_pos, detected_ranges):\n",
        "                    continue\n",
        "                if self._check_rejection_context(text, start_pos, end_pos):\n",
        "                    continue\n",
        "                \n",
        "                detected_ranges.append((start_pos, end_pos))\n",
        "                detections.append({\n",
        "                    'text': match.group(0),\n",
        "                    'start': start_pos,\n",
        "                    'end': end_pos,\n",
        "                    'label': label\n",
        "                })\n",
        "        \n",
        "        # Priority 4: Place-bound directions\n",
        "        place_bound_detections = self.detect_place_bound_directions(text, detected_ranges)\n",
        "        detections.extend(place_bound_detections)\n",
        "        \n",
        "        return detections\n",
        "    \n",
        "    def detect_both_either(self, text):\n",
        "        # Detect \"both/either direction(s)\" patterns\n",
        "        detections = []\n",
        "        \n",
        "        for match in self.both_either_pattern.finditer(text):\n",
        "            detections.append({\n",
        "                'text': match.group(0),\n",
        "                'start': match.start(),\n",
        "                'end': match.end(),\n",
        "                'label': 'BOTH_DIRECTIONS'\n",
        "            })\n",
        "        \n",
        "        return detections\n",
        "    \n",
        "    def detect_all(self, text):\n",
        "        # Run all detection rules on text\n",
        "        all_detections = []\n",
        "        all_detections.extend(self.detect_bound_directions(text))\n",
        "        all_detections.extend(self.detect_both_either(text))\n",
        "        return {'detections': all_detections}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Process Dataset\n",
        "\n",
        "Load the input data and apply direction detection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading Optional Spans/MTA_Data_preprocessed_routespans.csv...\n",
            "Loaded 226,160 records\n",
            "Loaded 226,160 records\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "INPUT_FILE = 'Optional Spans/MTA_Data_preprocessed_routespans.csv'\n",
        "OUTPUT_FILE = 'Preprocessed/MTA_Data_silver_directions.csv'\n",
        "\n",
        "# Load input data\n",
        "print(f\"Reading {INPUT_FILE}...\")\n",
        "df_input = pd.read_csv(INPUT_FILE)\n",
        "print(f\"Loaded {len(df_input):,} records\")\n",
        "\n",
        "# Initialize detector\n",
        "detector = DirectionDetector()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing headers...\n",
            "  Processed 10,000 / 226,160 records...\n",
            "  Processed 10,000 / 226,160 records...\n",
            "  Processed 20,000 / 226,160 records...\n",
            "  Processed 20,000 / 226,160 records...\n",
            "  Processed 30,000 / 226,160 records...\n",
            "  Processed 30,000 / 226,160 records...\n",
            "  Processed 40,000 / 226,160 records...\n",
            "  Processed 40,000 / 226,160 records...\n",
            "  Processed 50,000 / 226,160 records...\n",
            "  Processed 50,000 / 226,160 records...\n",
            "  Processed 60,000 / 226,160 records...\n",
            "  Processed 60,000 / 226,160 records...\n",
            "  Processed 70,000 / 226,160 records...\n",
            "  Processed 70,000 / 226,160 records...\n",
            "  Processed 80,000 / 226,160 records...\n",
            "  Processed 80,000 / 226,160 records...\n",
            "  Processed 90,000 / 226,160 records...\n",
            "  Processed 90,000 / 226,160 records...\n",
            "  Processed 100,000 / 226,160 records...\n",
            "  Processed 100,000 / 226,160 records...\n",
            "  Processed 110,000 / 226,160 records...\n",
            "  Processed 110,000 / 226,160 records...\n",
            "  Processed 120,000 / 226,160 records...\n",
            "  Processed 120,000 / 226,160 records...\n",
            "  Processed 130,000 / 226,160 records...\n",
            "  Processed 130,000 / 226,160 records...\n",
            "  Processed 140,000 / 226,160 records...\n",
            "  Processed 140,000 / 226,160 records...\n",
            "  Processed 150,000 / 226,160 records...\n",
            "  Processed 150,000 / 226,160 records...\n",
            "  Processed 160,000 / 226,160 records...\n",
            "  Processed 160,000 / 226,160 records...\n",
            "  Processed 170,000 / 226,160 records...\n",
            "  Processed 170,000 / 226,160 records...\n",
            "  Processed 180,000 / 226,160 records...\n",
            "  Processed 180,000 / 226,160 records...\n",
            "  Processed 190,000 / 226,160 records...\n",
            "  Processed 190,000 / 226,160 records...\n",
            "  Processed 200,000 / 226,160 records...\n",
            "  Processed 200,000 / 226,160 records...\n",
            "  Processed 210,000 / 226,160 records...\n",
            "  Processed 210,000 / 226,160 records...\n",
            "  Processed 220,000 / 226,160 records...\n",
            "  Processed 220,000 / 226,160 records...\n",
            "Completed processing 226,160 records\n",
            "Completed processing 226,160 records\n"
          ]
        }
      ],
      "source": [
        "# Process all headers\n",
        "print(\"Processing headers...\")\n",
        "\n",
        "direction_spans_list = []\n",
        "direction_list = []\n",
        "\n",
        "for idx, row in df_input.iterrows():\n",
        "    if (idx + 1) % 10000 == 0:\n",
        "        print(f\"  Processed {idx+1:,} / {len(df_input):,} records...\")\n",
        "    \n",
        "    header_text = row['header']\n",
        "    \n",
        "    # Handle missing or invalid headers\n",
        "    if pd.isna(header_text):\n",
        "        direction_spans_list.append(json.dumps([]))\n",
        "        direction_list.append(json.dumps(['UNSPECIFIED']))\n",
        "        continue\n",
        "    \n",
        "    header_text = str(header_text)\n",
        "    result = detector.detect_all(header_text)\n",
        "    detections = result['detections']\n",
        "    \n",
        "    if detections:\n",
        "        direction_spans = []\n",
        "        direction_labels = []\n",
        "        \n",
        "        for detection in detections:\n",
        "            direction_spans.append({\n",
        "                'start': detection['start'],\n",
        "                'end': detection['end'],\n",
        "                'type': 'DIRECTION',\n",
        "                'value': detection['label']\n",
        "            })\n",
        "            direction_labels.append(detection['label'])\n",
        "        \n",
        "        direction_spans_list.append(json.dumps(direction_spans))\n",
        "        direction_list.append(json.dumps(direction_labels))\n",
        "    else:\n",
        "        direction_spans_list.append(json.dumps([]))\n",
        "        direction_list.append(json.dumps(['UNSPECIFIED']))\n",
        "\n",
        "df_input['direction'] = direction_list\n",
        "df_input['direction_spans'] = direction_spans_list\n",
        "\n",
        "print(f\"Completed processing {len(df_input):,} records\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Save Output\n",
        "\n",
        "Write the labeled data to CSV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Writing output to Preprocessed/MTA_Data_silver_directions.csv...\n",
            "Successfully wrote 226,160 records to Preprocessed/MTA_Data_silver_directions.csv\n",
            "\n",
            "First 5 rows of output:\n",
            "Successfully wrote 226,160 records to Preprocessed/MTA_Data_silver_directions.csv\n",
            "\n",
            "First 5 rows of output:\n"
          ]
        },
        {
          "data": {
            "application/vnd.microsoft.datawrangler.viewer.v0+json": {
              "columns": [
                {
                  "name": "index",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "alert_id",
                  "rawType": "int64",
                  "type": "integer"
                },
                {
                  "name": "date",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "agency",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "status_label",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "affected",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "affected_spans",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "header",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "direction",
                  "rawType": "object",
                  "type": "string"
                },
                {
                  "name": "direction_spans",
                  "rawType": "object",
                  "type": "string"
                }
              ],
              "ref": "b6fad977-ceba-4e0e-9682-b00bd5ee5886",
              "rows": [
                [
                  "0",
                  "180128",
                  "11/05/2022 05:58:00 PM",
                  "NYCT Subway",
                  "delays",
                  "[\"A\", \"C\"]",
                  "[{\"start\": 0, \"end\": 1, \"type\": \"ROUTE\", \"value\": \"A\"}, {\"start\": 2, \"end\": 3, \"type\": \"ROUTE\", \"value\": \"C\"}]",
                  "A C trains are delayed while we conduct emergency track repairs in Manhattan.",
                  "[\"UNSPECIFIED\"]",
                  "[]"
                ],
                [
                  "1",
                  "189489",
                  "12/20/2022 07:09:00 PM",
                  "NYCT Subway",
                  "delays",
                  "[\"L\"]",
                  "[{\"start\": 0, \"end\": 1, \"type\": \"ROUTE\", \"value\": \"L\"}]",
                  "L trains are running with delays in both directions after NYPD completed an investigation at 6 Av.",
                  "[\"BOTH_DIRECTIONS\"]",
                  "[{\"start\": 36, \"end\": 51, \"type\": \"DIRECTION\", \"value\": \"BOTH_DIRECTIONS\"}]"
                ],
                [
                  "2",
                  "189321",
                  "12/20/2022 12:31:00 AM",
                  "NYCT Subway",
                  "delays",
                  "[\"J\"]",
                  "[{\"start\": 14, \"end\": 15, \"type\": \"ROUTE\", \"value\": \"J\"}]",
                  "Jamaica-bound J trains are delayed while we request EMS assistance for someone at Gates Av.",
                  "[\"PLACE_BOUND\"]",
                  "[{\"start\": 0, \"end\": 13, \"type\": \"DIRECTION\", \"value\": \"PLACE_BOUND\"}]"
                ],
                [
                  "3",
                  "188948",
                  "12/18/2022 06:12:00 AM",
                  "NYCT Subway",
                  "delays",
                  "[\"Q\"]",
                  "[{\"start\": 11, \"end\": 12, \"type\": \"ROUTE\", \"value\": \"Q\"}]",
                  "Southbound Q trains are running with delays after we addressed a door problem at DeKalb Av.",
                  "[\"SOUTHBOUND\"]",
                  "[{\"start\": 0, \"end\": 10, \"type\": \"DIRECTION\", \"value\": \"SOUTHBOUND\"}]"
                ],
                [
                  "4",
                  "187749",
                  "12/12/2022 02:26:00 PM",
                  "NYCT Subway",
                  "delays",
                  "[\"B\", \"C\"]",
                  "[{\"start\": 11, \"end\": 12, \"type\": \"ROUTE\", \"value\": \"B\"}, {\"start\": 13, \"end\": 14, \"type\": \"ROUTE\", \"value\": \"C\"}]",
                  "Southbound B C trains are running with delays after we addressed a door problem on a train at Cathedral Pkwy (110 St).",
                  "[\"SOUTHBOUND\"]",
                  "[{\"start\": 0, \"end\": 10, \"type\": \"DIRECTION\", \"value\": \"SOUTHBOUND\"}]"
                ]
              ],
              "shape": {
                "columns": 9,
                "rows": 5
              }
            },
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>alert_id</th>\n",
              "      <th>date</th>\n",
              "      <th>agency</th>\n",
              "      <th>status_label</th>\n",
              "      <th>affected</th>\n",
              "      <th>affected_spans</th>\n",
              "      <th>header</th>\n",
              "      <th>direction</th>\n",
              "      <th>direction_spans</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>180128</td>\n",
              "      <td>11/05/2022 05:58:00 PM</td>\n",
              "      <td>NYCT Subway</td>\n",
              "      <td>delays</td>\n",
              "      <td>[\"A\", \"C\"]</td>\n",
              "      <td>[{\"start\": 0, \"end\": 1, \"type\": \"ROUTE\", \"valu...</td>\n",
              "      <td>A C trains are delayed while we conduct emerge...</td>\n",
              "      <td>[\"UNSPECIFIED\"]</td>\n",
              "      <td>[]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>189489</td>\n",
              "      <td>12/20/2022 07:09:00 PM</td>\n",
              "      <td>NYCT Subway</td>\n",
              "      <td>delays</td>\n",
              "      <td>[\"L\"]</td>\n",
              "      <td>[{\"start\": 0, \"end\": 1, \"type\": \"ROUTE\", \"valu...</td>\n",
              "      <td>L trains are running with delays in both direc...</td>\n",
              "      <td>[\"BOTH_DIRECTIONS\"]</td>\n",
              "      <td>[{\"start\": 36, \"end\": 51, \"type\": \"DIRECTION\",...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>189321</td>\n",
              "      <td>12/20/2022 12:31:00 AM</td>\n",
              "      <td>NYCT Subway</td>\n",
              "      <td>delays</td>\n",
              "      <td>[\"J\"]</td>\n",
              "      <td>[{\"start\": 14, \"end\": 15, \"type\": \"ROUTE\", \"va...</td>\n",
              "      <td>Jamaica-bound J trains are delayed while we re...</td>\n",
              "      <td>[\"PLACE_BOUND\"]</td>\n",
              "      <td>[{\"start\": 0, \"end\": 13, \"type\": \"DIRECTION\", ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>188948</td>\n",
              "      <td>12/18/2022 06:12:00 AM</td>\n",
              "      <td>NYCT Subway</td>\n",
              "      <td>delays</td>\n",
              "      <td>[\"Q\"]</td>\n",
              "      <td>[{\"start\": 11, \"end\": 12, \"type\": \"ROUTE\", \"va...</td>\n",
              "      <td>Southbound Q trains are running with delays af...</td>\n",
              "      <td>[\"SOUTHBOUND\"]</td>\n",
              "      <td>[{\"start\": 0, \"end\": 10, \"type\": \"DIRECTION\", ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>187749</td>\n",
              "      <td>12/12/2022 02:26:00 PM</td>\n",
              "      <td>NYCT Subway</td>\n",
              "      <td>delays</td>\n",
              "      <td>[\"B\", \"C\"]</td>\n",
              "      <td>[{\"start\": 11, \"end\": 12, \"type\": \"ROUTE\", \"va...</td>\n",
              "      <td>Southbound B C trains are running with delays ...</td>\n",
              "      <td>[\"SOUTHBOUND\"]</td>\n",
              "      <td>[{\"start\": 0, \"end\": 10, \"type\": \"DIRECTION\", ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   alert_id                    date       agency status_label    affected  \\\n",
              "0    180128  11/05/2022 05:58:00 PM  NYCT Subway       delays  [\"A\", \"C\"]   \n",
              "1    189489  12/20/2022 07:09:00 PM  NYCT Subway       delays       [\"L\"]   \n",
              "2    189321  12/20/2022 12:31:00 AM  NYCT Subway       delays       [\"J\"]   \n",
              "3    188948  12/18/2022 06:12:00 AM  NYCT Subway       delays       [\"Q\"]   \n",
              "4    187749  12/12/2022 02:26:00 PM  NYCT Subway       delays  [\"B\", \"C\"]   \n",
              "\n",
              "                                      affected_spans  \\\n",
              "0  [{\"start\": 0, \"end\": 1, \"type\": \"ROUTE\", \"valu...   \n",
              "1  [{\"start\": 0, \"end\": 1, \"type\": \"ROUTE\", \"valu...   \n",
              "2  [{\"start\": 14, \"end\": 15, \"type\": \"ROUTE\", \"va...   \n",
              "3  [{\"start\": 11, \"end\": 12, \"type\": \"ROUTE\", \"va...   \n",
              "4  [{\"start\": 11, \"end\": 12, \"type\": \"ROUTE\", \"va...   \n",
              "\n",
              "                                              header            direction  \\\n",
              "0  A C trains are delayed while we conduct emerge...      [\"UNSPECIFIED\"]   \n",
              "1  L trains are running with delays in both direc...  [\"BOTH_DIRECTIONS\"]   \n",
              "2  Jamaica-bound J trains are delayed while we re...      [\"PLACE_BOUND\"]   \n",
              "3  Southbound Q trains are running with delays af...       [\"SOUTHBOUND\"]   \n",
              "4  Southbound B C trains are running with delays ...       [\"SOUTHBOUND\"]   \n",
              "\n",
              "                                     direction_spans  \n",
              "0                                                 []  \n",
              "1  [{\"start\": 36, \"end\": 51, \"type\": \"DIRECTION\",...  \n",
              "2  [{\"start\": 0, \"end\": 13, \"type\": \"DIRECTION\", ...  \n",
              "3  [{\"start\": 0, \"end\": 10, \"type\": \"DIRECTION\", ...  \n",
              "4  [{\"start\": 0, \"end\": 10, \"type\": \"DIRECTION\", ...  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Save to CSV\n",
        "print(f\"\\nWriting output to {OUTPUT_FILE}...\")\n",
        "df_input.to_csv(OUTPUT_FILE, index=False)\n",
        "print(f\"Successfully wrote {len(df_input):,} records to {OUTPUT_FILE}\")\n",
        "\n",
        "# Display first few rows\n",
        "print(\"\\nFirst 5 rows of output:\")\n",
        "df_input.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Writing output without spans to Preprocessed/MTA_Data_silver_no_spans.csv...\n",
            "Successfully wrote 226,160 records to Preprocessed/MTA_Data_silver_no_spans.csv\n",
            "Dropped columns: ['direction_spans', 'affected_spans']\n",
            "Successfully wrote 226,160 records to Preprocessed/MTA_Data_silver_no_spans.csv\n",
            "Dropped columns: ['direction_spans', 'affected_spans']\n"
          ]
        }
      ],
      "source": [
        "# Save version without span columns\n",
        "NO_SPANS_OUTPUT_FILE = 'Preprocessed/MTA_Data_silver_no_spans.csv'\n",
        "\n",
        "# Create a copy and drop span columns\n",
        "df_no_spans = df_input.copy()\n",
        "columns_to_drop = ['direction_spans']\n",
        "\n",
        "# Also drop affected_spans if it exists\n",
        "if 'affected_spans' in df_no_spans.columns:\n",
        "    columns_to_drop.append('affected_spans')\n",
        "\n",
        "df_no_spans = df_no_spans.drop(columns=columns_to_drop)\n",
        "\n",
        "print(f\"\\nWriting output without spans to {NO_SPANS_OUTPUT_FILE}...\")\n",
        "df_no_spans.to_csv(NO_SPANS_OUTPUT_FILE, index=False)\n",
        "print(f\"Successfully wrote {len(df_no_spans):,} records to {NO_SPANS_OUTPUT_FILE}\")\n",
        "print(f\"Dropped columns: {columns_to_drop}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
