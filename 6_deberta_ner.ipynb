{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeBERTa-v3 NER for MTA Service Alerts\n",
    "\n",
    "This notebook implements a Named Entity Recognition (NER) pipeline using `microsoft/deberta-v3-base` to extract `ROUTE` and `DIRECTION` entities from MTA transit alerts.\n",
    "It uses existing span annotations from the silver dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if missing\n",
    "# !pip install transformers torch pandas scikit-learn seqeval numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'AdamW' from 'transformers' (/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForTokenClassification, AdamW, get_linear_schedule_with_warmup\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseqeval\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m classification_report, f1_score\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'AdamW' from 'transformers' (/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from seqeval.metrics import classification_report, f1_score\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = \"Preprocessed/MTA_Data_silver_relations.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Parse JSON columns containing spans\n",
    "# The dataset contains stringified JSON, so we convert them back to objects\n",
    "df['affected_spans'] = df['affected_spans'].apply(json.loads)\n",
    "df['direction_spans'] = df['direction_spans'].apply(json.loads)\n",
    "\n",
    "# Convert date column for temporal splitting\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Display sample\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "df[['header', 'affected_spans', 'direction_spans']].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Label Map\n",
    "# 5 classes: O (Outside), B-ROUTE, I-ROUTE, B-DIRECTION, I-DIRECTION\n",
    "labels_to_ids = {\n",
    "    'O': 0,\n",
    "    'B-ROUTE': 1,\n",
    "    'I-ROUTE': 2,\n",
    "    'B-DIRECTION': 3,\n",
    "    'I-DIRECTION': 4\n",
    "}\n",
    "ids_to_labels = {v: k for k, v in labels_to_ids.items()}\n",
    "\n",
    "print(\"Label Map:\", labels_to_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Class & Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTANERDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len=128):\n",
    "        self.data = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.label_map = labels_to_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.data.iloc[index]\n",
    "        text = row['header']\n",
    "        \n",
    "        # Combine spans\n",
    "        # Structure: [{'start': 0, 'end': 1, 'type': 'ROUTE', ...}]\n",
    "        spans = row['affected_spans'] + row['direction_spans']\n",
    "\n",
    "        # Tokenize text\n",
    "        # return_offsets_mapping=True gives us character start/end for each token\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            return_offsets_mapping=True,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Get offsets (remove batch dimension)\n",
    "        offset_mapping = encoding['offset_mapping'][0].tolist()\n",
    "        \n",
    "        # Initialize labels with 'O' (0)\n",
    "        # -100 is used by PyTorch to ignore loss for special tokens (CLS, SEP, PAD)\n",
    "        labels = [0] * len(offset_mapping)\n",
    "\n",
    "        # Create BIO tags\n",
    "        # Iterate over tokens and check if they fall inside any span\n",
    "        for idx, (start, end) in enumerate(offset_mapping):\n",
    "            # Skip special tokens (start=0, end=0 usually implies special or empty)\n",
    "            # But be careful: (0,0) could be valid for some tokenizers, \n",
    "            # DeBERTa uses (0,0) for CLS/SEP/PAD usually.\n",
    "            if start == end:\n",
    "                labels[idx] = -100\n",
    "                continue\n",
    "\n",
    "            # Check each span\n",
    "            token_label = 'O'\n",
    "            for span in spans:\n",
    "                span_start = span['start']\n",
    "                span_end = span['end']\n",
    "                span_type = span['type']  # ROUTE or DIRECTION\n",
    "\n",
    "                # Check overlap\n",
    "                if start >= span_start and end <= span_end:\n",
    "                    # If this token starts at the beginning of the span, it's B-\n",
    "                    # Note: strict equality (start == span_start) might miss subwords \n",
    "                    # if tokenizer splits differently. \n",
    "                    # We use a simple logic: if it's the first token encountered for this span, B-, else I-\n",
    "                    # Better logic: if start == span_start -> B, else I.\n",
    "                    \n",
    "                    if start == span_start:\n",
    "                        token_label = f\"B-{span_type}\"\n",
    "                    else:\n",
    "                        token_label = f\"I-{span_type}\"\n",
    "                    break # Found a label, stop checking spans\n",
    "            \n",
    "            if token_label != 'O':\n",
    "                labels[idx] = self.label_map[token_label]\n",
    "\n",
    "        # Convert to tensor\n",
    "        item = {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(labels, dtype=torch.long)\n",
    "        }\n",
    "        return item\n",
    "\n",
    "# Initialize Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-v3-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Temporal Data Splits\n",
    "\n",
    "Implement true temporal split without shuffling to prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by date for temporal split\n",
    "df_sorted = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "# Compute complexity metric for analysis (optional - not used for splitting)\n",
    "df_sorted['num_dirs'] = df_sorted['direction_spans'].apply(len)\n",
    "df_sorted['num_routes'] = df_sorted['affected_spans'].apply(len)\n",
    "df_sorted['complexity_bin'] = pd.cut(\n",
    "    df_sorted['num_dirs'] + df_sorted['num_routes'], \n",
    "    bins=[-1, 0, 1, 2, float('inf')], \n",
    "    labels=['none', 'single', 'double', 'multi']\n",
    ")\n",
    "\n",
    "print(\"Complexity distribution:\")\n",
    "print(df_sorted['complexity_bin'].value_counts())\n",
    "\n",
    "# TRUE Temporal Split: 70% Train, 15% Val, 15% Test (NO SHUFFLING)\n",
    "n = len(df_sorted)\n",
    "train_end = int(n * 0.70)\n",
    "val_end = int(n * 0.85)\n",
    "\n",
    "train_df = df_sorted.iloc[:train_end].reset_index(drop=True)\n",
    "val_df = df_sorted.iloc[train_end:val_end].reset_index(drop=True)\n",
    "test_df = df_sorted.iloc[val_end:].reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nSplit sizes: Train: {len(train_df):,}, Val: {len(val_df):,}, Test: {len(test_df):,}\")\n",
    "\n",
    "# Verify temporal ordering\n",
    "print(f\"\\nDate ranges:\")\n",
    "print(f\"  Train: {train_df['date'].min()} to {train_df['date'].max()}\")\n",
    "print(f\"  Val:   {val_df['date'].min()} to {val_df['date'].max()}\")\n",
    "print(f\"  Test:  {test_df['date'].min()} to {test_df['date'].max()}\")\n",
    "\n",
    "# Show complexity distribution per split (for analysis only)\n",
    "print(\"\\nComplexity distribution per split:\")\n",
    "for name, split_df in [(\"Train\", train_df), (\"Val\", val_df), (\"Test\", test_df)]:\n",
    "    dist = split_df['complexity_bin'].value_counts(normalize=True) * 100\n",
    "    print(f\"{name}: {dict(dist.round(1))}\")\n",
    "\n",
    "# Create Datasets & DataLoaders\n",
    "train_dataset = MTANERDataset(train_df, tokenizer)\n",
    "val_dataset = MTANERDataset(val_df, tokenizer)\n",
    "test_dataset = MTANERDataset(test_df, tokenizer)\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Rule-Based Baseline\n",
    "\n",
    "Simple regex-based extractor to establish baseline performance and verify data isn't trivially solvable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Class Weights for Imbalance\n",
    "# We count occurrences of each label in the training set\n",
    "label_counts = {id: 0 for id in labels_to_ids.values()}\n",
    "\n",
    "# Sample a subset to estimate weights to avoid iterating full dataset if huge\n",
    "# Or iterate full dataset (safe for 200k rows)\n",
    "print(\"Calculating class weights...\")\n",
    "for i in tqdm(range(len(train_dataset))):\n",
    "    labels = train_dataset[i]['labels'].numpy()\n",
    "    # Filter out -100\n",
    "    valid_labels = labels[labels != -100]\n",
    "    for label in valid_labels:\n",
    "        label_counts[label] += 1\n",
    "\n",
    "total_counts = sum(label_counts.values())\n",
    "num_classes = len(labels_to_ids)\n",
    "\n",
    "print(\"\\nLabel distribution:\")\n",
    "for label_name, label_id in labels_to_ids.items():\n",
    "    count = label_counts[label_id]\n",
    "    pct = (count / total_counts) * 100 if total_counts > 0 else 0\n",
    "    print(f\"  {label_name}: {count:,} ({pct:.2f}%)\")\n",
    "\n",
    "# Inverse frequency weights with smoothing and manual boosting for rare classes\n",
    "# Base weights: total / (num_classes * count)\n",
    "class_weights = []\n",
    "for i in range(num_classes):\n",
    "    count = label_counts[i]\n",
    "    if count > 0:\n",
    "        # Smoothed inverse frequency\n",
    "        weight = total_counts / (num_classes * count)\n",
    "        # Cap weights to prevent extreme values\n",
    "        weight = min(weight, 10.0)\n",
    "    else:\n",
    "        weight = 1.0\n",
    "    class_weights.append(weight)\n",
    "\n",
    "# Manual adjustments based on analysis:\n",
    "# - Boost B-DIRECTION (idx 3) for better recall on direction starts\n",
    "# - Boost I-ROUTE (idx 2) and I-DIRECTION (idx 4) for multi-token entities\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "# Apply additional boost for rare continuation tags\n",
    "boost_factors = torch.tensor([1.0, 1.0, 1.5, 1.5, 1.5], dtype=torch.float)  # O, B-ROUTE, I-ROUTE, B-DIR, I-DIR\n",
    "class_weights = class_weights * boost_factors\n",
    "\n",
    "print(\"\\nFinal Class Weights:\")\n",
    "for label_name, label_id in labels_to_ids.items():\n",
    "    print(f\"  {label_name}: {class_weights[label_id]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Configuration & Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('mps') if torch.backends.mps.is_available() else torch.device('cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load Model\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    'microsoft/deberta-v3-base',\n",
    "    num_labels=len(labels_to_ids)\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer with different learning rates\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': model.deberta.parameters(), 'lr': 3e-5},\n",
    "    {'params': model.classifier.parameters(), 'lr': 1e-4}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters)\n",
    "\n",
    "# Scheduler\n",
    "epochs = 3 # Start with small number for demo, user can increase\n",
    "total_steps = len(train_loader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=int(0.1 * total_steps), \n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Loss Function with Weights\n",
    "loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, optimizer, scheduler, device, loss_fct):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(data_loader, desc=\"Training\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Calculate loss manually to apply class weights\n",
    "        # Flatten logits and labels\n",
    "        active_loss = attention_mask.view(-1) == 1\n",
    "        active_logits = logits.view(-1, len(labels_to_ids))\n",
    "        active_labels = labels.view(-1)\n",
    "        \n",
    "        # Only compute loss for active labels (not -100 and not padded)\n",
    "        # PyTorch CrossEntropy handles -100 ignoring automatically if we pass it,\n",
    "        # but we also want to apply our class weights.\n",
    "        loss = loss_fct(active_logits, active_labels)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient Clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def eval_epoch(model, data_loader, device, loss_fct):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            loss = loss_fct(logits.view(-1, len(labels_to_ids)), labels.view(-1))\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Get predictions\n",
    "            preds = torch.argmax(logits, dim=2)\n",
    "            \n",
    "            # Convert to list and handle ignores (-100)\n",
    "            for i in range(len(labels)):\n",
    "                true_labels = labels[i]\n",
    "                pred_labels = preds[i]\n",
    "                \n",
    "                true_list = []\n",
    "                pred_list = []\n",
    "                \n",
    "                for j in range(len(true_labels)):\n",
    "                    if true_labels[j] != -100:\n",
    "                        true_list.append(ids_to_labels[true_labels[j].item()])\n",
    "                        pred_list.append(ids_to_labels[pred_labels[j].item()])\n",
    "                \n",
    "                all_labels.append(true_list)\n",
    "                all_preds.append(pred_list)\n",
    "                \n",
    "    return total_loss / len(data_loader), all_labels, all_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_f1 = 0\n",
    "patience = 3\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "    \n",
    "    train_loss = train_epoch(model, train_loader, optimizer, scheduler, device, loss_fct)\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    \n",
    "    val_loss, val_labels, val_preds = eval_epoch(model, val_loader, device, loss_fct)\n",
    "    \n",
    "    # Compute Metrics\n",
    "    f1 = f1_score(val_labels, val_preds)\n",
    "    report = classification_report(val_labels, val_preds)\n",
    "    \n",
    "    print(f\"Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"Val F1: {f1:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "    \n",
    "    # Save Best Model & Early Stopping\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        patience_counter = 0\n",
    "        # Save model\n",
    "        model.save_pretrained(\"models/deberta_ner_best\")\n",
    "        tokenizer.save_pretrained(\"models/deberta_ner_best\")\n",
    "        print(\"New best model saved!\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"No improvement. Patience: {patience_counter}/{patience}\")\n",
    "        \n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation on Test Set\n",
    "\n",
    "Evaluate the saved best NER model on the held-out test split using span-aware metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import precision_score, recall_score\n",
    "\n",
    "best_dir = \"models/deberta_ner_best\"\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(best_dir, fix_mistral_regex=True)\n",
    "eval_model = AutoModelForTokenClassification.from_pretrained(best_dir)\n",
    "eval_model.to(device)\n",
    "\n",
    "test_dataset = MTANERDataset(test_df, eval_tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "def evaluate_ner(model, data_loader, device):\n",
    "    loss_fct_eval = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "    all_true = []\n",
    "    all_pred = []\n",
    "    total_loss = 0.0\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating test\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            loss = loss_fct_eval(logits.view(-1, len(labels_to_ids)), labels.view(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(logits, dim=2)\n",
    "\n",
    "            for i in range(len(labels)):\n",
    "                true_seq = []\n",
    "                pred_seq = []\n",
    "                for j in range(len(labels[i])):\n",
    "                    if labels[i][j].item() == -100:\n",
    "                        continue\n",
    "                    true_seq.append(ids_to_labels[labels[i][j].item()])\n",
    "                    pred_seq.append(ids_to_labels[preds[i][j].item()])\n",
    "                all_true.append(true_seq)\n",
    "                all_pred.append(pred_seq)\n",
    "\n",
    "    num_batches = max(len(data_loader), 1)\n",
    "    return {\n",
    "        \"loss\": total_loss / num_batches,\n",
    "        \"precision\": precision_score(all_true, all_pred),\n",
    "        \"recall\": recall_score(all_true, all_pred),\n",
    "        \"f1\": f1_score(all_true, all_pred),\n",
    "        \"report\": classification_report(all_true, all_pred)\n",
    "    }\n",
    "\n",
    "\n",
    "test_metrics = evaluate_ner(eval_model, test_loader, device)\n",
    "print(f\"Test loss: {test_metrics['loss']:.4f}\")\n",
    "print(f\"Test precision: {test_metrics['precision']:.4f}\")\n",
    "print(f\"Test recall: {test_metrics['recall']:.4f}\")\n",
    "print(f\"Test F1: {test_metrics['f1']:.4f}\")\n",
    "print(test_metrics[\"report\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Inference Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ner(text, model, tokenizer, device):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        preds = torch.argmax(logits, dim=2)[0]\n",
    "    \n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "    entities = []\n",
    "    \n",
    "    for token, label_id in zip(tokens, preds):\n",
    "        label = ids_to_labels[label_id.item()]\n",
    "        if label != \"O\" and token not in [\"[CLS]\", \"[SEP]\"]:\n",
    "            entities.append((token, label))\n",
    "            \n",
    "    return entities\n",
    "\n",
    "active_model = eval_model if \"eval_model\" in globals() else model\n",
    "active_tokenizer = eval_tokenizer if \"eval_tokenizer\" in globals() else tokenizer\n",
    "\n",
    "# Test\n",
    "test_text = \"Jamaica-bound J trains are delayed\"\n",
    "print(predict_ner(test_text, active_model, active_tokenizer, device))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
